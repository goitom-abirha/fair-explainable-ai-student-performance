# ğŸ“ Fair and Explainable AI: Bias Detection and Transparency in Student Performance Prediction

>  *Independent Research Project by Goitom Abirha, exploring fairness and explainability in educational machine learning systems.*

![Python](https://img.shields.io/badge/Python-3.12-blue)
![scikit-learn](https://img.shields.io/badge/scikit--learn-ML-orange)
![Fairness](https://img.shields.io/badge/Fair--AI-Responsible%20ML-green)
![License](https://img.shields.io/badge/license-MIT-lightgrey)

---

##  Overview
This self-directed research project investigates **fairness and interpretability** in machine-learning models predicting student academic performance.  
It applies **bias detection**, **fairness auditing**, and **Explainable AI (XAI)** techniques to promote ethical and transparent AI in education systems.

The research explores how data-driven models can be made more **fair, transparent, and socially responsible**, supporting equitable student success.

---

##  Dataset
- **Source:** [UCI Machine Learning Repository â€“ Student Performance Dataset](https://archive.ics.uci.edu/ml/datasets/student+performance)
- **Features:** Demographic, social, and academic variables (e.g., age, gender, study time, absences, parental education, G1â€“G3 grades)
- **Target Variable:** Final grade (G3)
- **Sensitive Attributes:** Gender, parental education, family income (used to assess fairness)

---

##  Methodology
1. **Exploratory Data Analysis (EDA):** Identify hidden bias and visualize group disparities  
2. **Data Preparation:** Handle missing data, encode categorical variables, normalize features, and balance data using SMOTE  
3. **Model Training:** Compare multiple algorithms (Logistic Regression, Random Forest, SVM)  
4. **Fairness Auditing:** Evaluate bias using fairness metrics (Demographic Parity, Equal Opportunity, Disparate Impact) with IBM AIF360  
5. **Explainability:** Apply SHAP and LIME to interpret predictions  
6. **Deployment:** Develop a Streamlit dashboard to visualize fairness and interpretability results  

---

##  Tools & Libraries
| Category | Tools |
|-----------|--------|
| **Programming** | Python 3.12 |
| **Data Processing** | pandas, numpy |
| **Modeling** | scikit-learn, imblearn |
| **Fairness & Ethics** | IBM AIF360, fairness-metrics |
| **Explainability (XAI)** | SHAP, LIME, PDPbox |
| **Visualization** | matplotlib, seaborn, plotly |
| **Deployment** | Streamlit, Flask |
| **Version Control** | GitHub |

---

---

##  Repository Structure

fair-explainable-ai-student-performance/
â”‚
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda_bias_analysis.ipynb
â”‚ â”œâ”€â”€ 02_data_preprocessing.ipynb
â”‚ â”œâ”€â”€ 03_model_training.ipynb
â”‚ â”œâ”€â”€ 04_fairness_evaluation.ipynb
â”‚ â””â”€â”€ 05_explainability_shap_lime.ipynb
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ preprocessing.py
â”‚ â”œâ”€â”€ fairness_metrics.py
â”‚ â”œâ”€â”€ explainability_tools.py
â”‚ â””â”€â”€ app.py
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ (link to UCI dataset)
â”‚ â””â”€â”€ processed/
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ eda_visuals/
â”‚ â”œâ”€â”€ fairness_metrics.csv
â”‚ â”œâ”€â”€ shap_visuals/
â”‚ â””â”€â”€ model_performance.csv
â”‚
â”œâ”€â”€ docs/
â”‚ â”œâ”€â”€ Research_Proposal_Fair_and_Explainable_AI_Final.pdf
â”‚ â”œâ”€â”€ Presentation_Slides.pdf
â”‚ â””â”€â”€ Screenshots/
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md



---

## ğŸ—“ï¸ 7-Week Project Roadmap
| **Week** | **Focus Area** | **Deliverables / Files** |
|-----------|----------------|--------------------------|
| Week 1 | Project setup & documentation | Repository structure, proposal, README.md |
| Week 2 | EDA & bias detection | `01_eda_bias_analysis.ipynb` |
| Week 3 | Data preprocessing & feature engineering | `02_data_preprocessing.ipynb` |
| Week 4 | Model training | `03_model_training.ipynb` |
| Week 5 | Fairness evaluation | `04_fairness_evaluation.ipynb` |
| Week 6 | Explainability (SHAP, LIME) | `05_explainability_shap_lime.ipynb` |
| Week 7 | Dashboard & final deployment | `src/app.py`, Streamlit link, documentation |

---

## ğŸ“ˆ Results (Expected Outcomes)
- Quantified bias across gender, parental education, and socioeconomic features  
- Fairness-audited models balancing accuracy and equity  
- Transparent SHAP and LIME explanations for model predictions  
- Interactive Streamlit dashboard demonstrating fairness metrics and interpretability

---

##  Demo
ğŸ”— **[Live Streamlit App â€“ Coming Soon](https://yourappname.streamlit.app)** 

---

##  Example Visuals
| SHAP Summary Plot | Fairness Dashboard |
|--------------------|-------------------|
| ![SHAP Plot](docs/Screenshots/shap_summary.png) | ![Dashboard](docs/Screenshots/dashboard.png) |

---

##  Research Ownership
This project was **conceptualized, developed, and documented independently** by **Goitom Abirha** as part of ongoing professional research in **Responsible and Explainable AI**.  
No external supervision or institutional funding was involved.

---
##  Research Proposal
[![View PDF](https://img.shields.io/badge/View--PDF-Fair%20%26%20Explainable%20AI-blue)](docs/Research_Proposal_Fair_and_Explainable_AI_Final.pdf)
---

##  References
- Barocas, S., Hardt, M., & Narayanan, A. (2019). *Fairness and Machine Learning.*  
- Cortez, P., & Silva, A. (2008). *Using Data Mining to Predict Secondary School Student Performance.*  
- Lundberg, S. M., & Lee, S.-I. (2017). *A Unified Approach to Interpreting Model Predictions.* *NeurIPS.*  
- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). *â€œWhy Should I Trust You?â€* *KDD Conference.*

---

##  Author
**Goitom Abirha**  
 M.Sc. in Data Science â€” Eastern University  
 Silver Spring, Maryland, USA  

ğŸ“§ [goitomabirha41@gmail.com](mailto:goitomabirha41@gmail.com)  
ğŸ”— [LinkedIn Profile](https://www.linkedin.com/in/goitom-abirha-089428397)



---

##  License
This project is licensed under the **MIT License** â€” free to use and modify with proper credit.


## Week 2 â€” Exploratory Data Analysis & Bias Detection (Final Summary)
 Objective

The goal of Week 2 was to perform a detailed exploratory analysis (EDA) of the Student Performance dataset and identify early signs of bias across demographic and socio-economic groups. This step establishes the foundation for building fair and explainable machine-learning models in later weeks.

 Data Overview

â€¢	Combined dataset size: 1044 students, 33 features
â€¢	Two original datasets merged: Math and Portuguese
â€¢	No missing values
â€¢	16 numeric variables and 17 categorical variables

Key Analyses Performed
1. Univariate Exploration
â€¢	Histograms for numeric features (age, absences, studytime, grades)
â€¢	Bar plots for categorical features (sex, school, parental education, internet)
â€¢	Visuals saved to figures/eda/
â€¢	These plots help understand the distribution and balance of the dataset.

2. Correlation Analysis

â€¢	Generated a correlation heatmap for all numeric features
â€¢	Found strongest predictors of final grade (G3):

o	G2 (previous grade)
o	G1
o	Studytime
o	Failures (negative correlation)
This helps guide feature selection for future modeling.

3. Bias-Oriented Group Analysis

Examined mean G3 across key groups:

Attribute	                              Gap Between Groups	              Key Observation
Mother's Education (Medu)	      2.33 points                             	 largest difference
Father's Education (Fedu)	      1.97 points	                                    Strong socio-economic factor
School (GP vs MS)	                  1.12 points     	                        GP students perform higher
Internet Access	                  1.02 points	                                    Digital divide effect
Gender                                          0.24 points     	                        smallest gap

These findings suggest that socio-economic factors show stronger disparities than gender or family structure.

 Fairness Insights

â€¢	Early indicators of potential unfairness include:
â€¢	Higher grades for students with more educated parents
â€¢	Performance difference between schools (GP vs MS)
â€¢	Performance advantage for students with internet access
â€¢	Lower performance among students in romantic relationships (possible stress/time constraint?)
These disparities will guide Feature Engineering (Week 3) and Fairness Auditing (Week 5).
Outputs Generated

â€¢	figures/eda/*.png â€” all histograms, bar plots, boxplots, heatmaps
â€¢	reports/group_target_stats.csv â€” descriptive stats by group
â€¢	reports/initial_bias_gaps.csv â€” gap analysis
â€¢	reports/eda_bias_notes.md â€” auto-generated narrative report
â€¢	Updated notebook: 01_eda_bias_analysis.ipynb

